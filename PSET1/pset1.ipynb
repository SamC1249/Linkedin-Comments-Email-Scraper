{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d7c41cb8",
      "metadata": {
        "id": "d7c41cb8"
      },
      "source": [
        "# Problem Set 1 — Four Problems in Dynamic Programming\n",
        "\n",
        "*Markov Decision Processes & DP Methods*\n",
        "\n",
        "**Due: September 26 (Friday), 11:59pm**\n",
        "\n",
        "### Problem 1 — Inscribed Polygon of Maximum Perimeter (Pen & Paper)\n",
        "TODO list:\n",
        "- (a) show Q-value function $Q_{N-1}$ (6pt)\n",
        "- (b) show convexity (6pt)\n",
        "- (c) show optimal control signal $u_{N-1}$ (6pt)\n",
        "- (d) induction to any $k$-th step Q-function $Q_k$ (6pt)\n",
        "- (e) show all optimal control signal $u_k$ (6pt)\n",
        "\n",
        "Bonus:\n",
        "- (f) show convexity (5pt)\n",
        "- (g)(coding) solve the problem using optimization (5pt)\n",
        "### Problem 2 — Proof of convergence of value iteration (Pen & Paper)\n",
        "TODO list:\n",
        "- 2.1 contraction of bellman operator (5pt)\n",
        "- 2.2 linear convergence (5pt)\n",
        "- 2.3 stoping criteria (5pt)\n",
        "- 2.4 iteration bound (5pt)\n",
        "### Problem 3 — Cliffwalk (coding)\n",
        "TODO list:\n",
        "- 3.2 fill in code for policy evaluation (10pt)\n",
        "- 3.3 fill in code for policy iteration (10pt)\n",
        "- 3.4 fill in code for value iteration (10pt)\n",
        "\n",
        "### Problem 4 — Matrix–Vector Representation of DP\n",
        "TODO list:\n",
        "- 4.1 build the transition matrix $P$ (5pt)\n",
        "- 4.2 write bellman equation as matrix form (5pt)\n",
        "- 4.3 solve the matrix equation by fix-point iteration (10pt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install numpy matplotlib tqdm gymnasium cvxpy\n"
      ],
      "metadata": {
        "id": "19LAlQ_5mfT4",
        "outputId": "ea0d8957-5d99-4b0d-eb6f-e053d5de6717",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "19LAlQ_5mfT4",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.12/dist-packages (1.6.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.12/dist-packages (from cvxpy) (1.0.4)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from cvxpy) (0.11.1)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.12/dist-packages (from cvxpy) (3.2.8)\n",
            "Requirement already satisfied: scipy>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from cvxpy) (1.16.1)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.12/dist-packages (from clarabel>=0.5.0->cvxpy) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from osqp>=0.6.2->cvxpy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from osqp>=0.6.2->cvxpy) (75.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from osqp>=0.6.2->cvxpy) (1.5.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi->clarabel>=0.5.0->cvxpy) (2.23)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->osqp>=0.6.2->cvxpy) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "999f44b8",
      "metadata": {
        "id": "999f44b8"
      },
      "source": [
        "## 1. Inscribed Polygon of Maximal Perimeter (Pen and Paper)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bccd1ea",
      "metadata": {
        "id": "8bccd1ea"
      },
      "source": [
        "In lectures, we have seen how dynamic programming (DP) can compute optimal value functions and optimal policies for finite-horizon MDPs with discrete state space and action space (i.e., the tabular case).\n",
        "\n",
        "In this exercise, we will see that DP can also solve an optimal control problem with continuous state space and action space.\n",
        "This problem is a geometry problem where we try to find the $N$-side polygon inscribed inside a circle with maximum perimeter. We will walk you through the key steps of formulating and solving the problem, while leaving a few mathematical details for you to fill in.\n",
        "\n",
        "Given a circle with radius $1$, we can randomly choose $N$ distinct points on the circle to form a polygon with $N$ vertices and sides, as shown in Fig. 1 with $N=3,4,5$.\n",
        "\n",
        "<figure style=\"text-align:center;\">\n",
        "  <img src=\"https://raw.githubusercontent.com/ComputationalRobotics/2025-ES-AM-158-PSET/main/PSET1/polygon-inside-circle.png\" width=\"600\" alt=\"Inscribed polygon\">\n",
        "  <figcaption style=\"color:#6a737d; font-style:italic;\">\n",
        "    Figure 1. Polygons inscribed inside a circle\n",
        "  </figcaption>\n",
        "</figure>\n",
        "\n",
        "Once the $N$ points are chosen, the $N$-polygon will have a perimeter, i.e., the sum of the lengths of its edges.\n",
        "\n",
        "What is the configuration of the $N$ points such that the resulting $N$-polygon has the maximum perimeter? We claim that the answer is when the $N$-polygon has edges of equal lengths, or in other words, when the $N$ points are placed on the circle evenly.\n",
        "\n",
        "Let us use dynamic programming to prove the claim.\n",
        "\n",
        "To use dynamic programming, we need to define a dynamical system and a reward function.\n",
        "\n",
        "<figure style=\"text-align:center;\">\n",
        "  <img src=\"https://raw.githubusercontent.com/ComputationalRobotics/2025-ES-AM-158-PSET/main/PSET1/sequential-placement-N-point.png\" width=\"360\" alt=\"Inscribed polygon\">\n",
        "  <figcaption style=\"color:#6a737d; font-style:italic;\">\n",
        "    Figure 2. Sequential placement of N points on the circle.\n",
        "  </figcaption>\n",
        "</figure>\n",
        "\n",
        "**Dynamical system.**\n",
        "\n",
        "\n",
        "We will use $\\{x_1, \\ldots, x_N\\}$ to denote the angular positions of the $N$ points to be placed on the circle (with slight abuse of notation, we will call each of those points $x_k$ as well). In particular, as shown in Fig. 2, let us use $x_k$ to denote the angle between the line $O — x_k$ and the vertical line (O is the center of the circle), with zero angle starting at 12 o’clock and clockwise being positive. Without loss of generality, we assume $x_1 = 0$. (if $x_1$ is nonzero, we can always rotate the entire circle so that $x_1 = 0$).\n",
        "\n",
        "After the $k$-th point is placed, we can “control” where the next point $x_{k+1}$ will be, by deciding the incremental angle between $x_{k+1}$ and $x_k$, denoted as $u_k > 0$ in Fig. 2. This is simply saying the dynamics is\n",
        "\n",
        "$$\n",
        "x_{k+1} = x_k + u_k, \\quad k=1,\\ldots,N-1, \\quad x_1 = 0.\n",
        "$$\n",
        "\n",
        "Notice here we did not use an MDP to formulate this problem because the dynamics is deterministic. In the MDP language, this would correspond to, at time step $k$, if the agent takes action $u_k$ at state $x_k$, then the probability of transitioning to state $x_k + u_k$ at time $k+1$ is $1$, and the probability of transitioning to other states is zero.\n",
        "\n",
        "\n",
        "**Reward.**\n",
        "\n",
        "\n",
        "The perimeter of the $N$-polygon is therefore\n",
        "\n",
        "$$\n",
        "g_N(x_N) + \\sum_{k=1}^{N-1} g_k(x_k,u_k),\n",
        "$$\n",
        "\n",
        "with the terminal reward\n",
        "\n",
        "$$\n",
        "g_N(x_N) = 2 \\sin\\left(\\frac{2\\pi-x_N}{2}\\right),\n",
        "$$\n",
        "\n",
        "the distance between $x_N$ and $x_1$ (see Fig. 2), and the running reward\n",
        "\n",
        "$$\n",
        "g_k(x_k,u_k) = 2 \\sin\\left(\\frac{u_k}{2}\\right).\n",
        "$$\n",
        "\n",
        "**Dynamic programming.**\n",
        "\n",
        "We are now ready to invoke dynamic programming. Recall in lectures the key steps of DP are first to initialize the optimal value functions at the terminal time $k=N$, and then perform backward recursion to compute the optimal value functions at time $k=N-1,\\dots,1$.\n",
        "\n",
        "We start by setting\n",
        "\n",
        "$$\n",
        "V_N(x_N) = g_N(x_N) = 2 \\sin\\left(\\frac{2\\pi-x_N}{2}\\right).\n",
        "$$\n",
        "\n",
        "Unlike in lectures where we initialized the terminal value functions as all zero, here we initialize the terminal value functions as $g_N(x_N)$ because there is a \"terminal-state\" reward.\n",
        "\n",
        "We then compute $V_{N-1}(x_{N-1})$ as\n",
        "\n",
        "$$\n",
        "V_{N-1}(x_{N-1})\n",
        "= \\max_{0 < u_{N-1} < 2\\pi-x_{N-1}}\n",
        "\\left\\{\n",
        "  \\underbrace{ 2 \\sin\\left(\\tfrac{u_{N-1}}{2}\\right) + V_N(x_{N-1} + u_{N-1}) }_{Q_{N-1}(x_{N-1}, u_{N-1})}\n",
        "\\right\\},\n",
        "\\tag{9.1}\n",
        "$$\n",
        "\n",
        "where $u_{N-1} < 2\\pi-x_{N-1}$ because we do not want $x_N$ to cross $2\\pi$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c0764d9",
      "metadata": {
        "id": "4c0764d9"
      },
      "source": [
        "\n",
        "**a**. Show that\n",
        "\n",
        "$$\n",
        "Q_{N-1}(x_{N-1}, u_{N-1})\n",
        "= 2 \\sin\\left(\\tfrac{u_{N-1}}{2}\\right)\n",
        "+ 2 \\sin\\left(\\tfrac{2\\pi-x_{N-1}-u_{N-1}}{2}\\right),\n",
        "$$\n",
        "\n",
        "and thus\n",
        "\n",
        "$$\n",
        "\\frac{\\partial Q_{N-1}(x_{N-1}, u_{N-1})}{\\partial u_{N-1}}\n",
        "= \\cos\\left(\\tfrac{u_{N-1}}{2}\\right)\n",
        "- \\cos\\left(\\tfrac{2\\pi-x_{N-1}-u_{N-1}}{2}\\right).\n",
        "$$\n",
        "\n",
        "**(TODO) ANSWER:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just going to add my thoughts first. We start at some function V and then Q. This seems like an application of the Bellman Consistency from class. I'll need to look into it. The specific wording after looking at the notes is: state-value function -> action-value function. Don't see the logic from  Q_{N-1} to alpha Q_{n-1}."
      ],
      "metadata": {
        "id": "_neYZqVJoIP-"
      },
      "id": "_neYZqVJoIP-"
    },
    {
      "cell_type": "markdown",
      "id": "a666ead0",
      "metadata": {
        "id": "a666ead0"
      },
      "source": [
        "\n",
        "\n",
        "**b**. Show that $Q_{N-1}(x_{N-1}, u_{N-1})$ is concave (i.e., $-Q_{N-1}(x_{N-1}, u_{N-1})$ is convex) in $u_{N-1}$ for every $x_{N-1} \\in (0, \\pi)$ and $u_{N-1} \\in (0, 2\\pi-x_{N-1})$.\n",
        "(Hint: compute the second derivative of $Q_{N-1}(x_{N-1}, u_{N-1})$ with respect to $u_{N-1}$ and verify it is positive definite)\n",
        "\n",
        "**(TODO) ANSWER:** $$ Q_{N-1}(x_{N-1}, u_{N-1})du=u_{N-1} \\cdot Q_{N-1}(x_{N-1},u_{N-1})$$\n",
        "\n",
        "Then, we take the derivative again: $$u_{N-1} \\cdot Q_{N-1}(x_{N-1},u_{N-1})  du = u_{N-1}^2 \\cdot Q_{N-1}(x_{N-1},u_{N-1}) $$\n",
        "\n",
        "Since $u_{N-1}^2$ is always positive, we need to verify that the $Q_{N-1}$ term is positive as well."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concave- second derivative is positive definite.\n"
      ],
      "metadata": {
        "id": "t-1MDKbQp3xe"
      },
      "id": "t-1MDKbQp3xe"
    },
    {
      "cell_type": "markdown",
      "id": "a71bd6ca",
      "metadata": {
        "id": "a71bd6ca"
      },
      "source": [
        "\n",
        "\n",
        "**c**. With a and b, show that the optimal $u_{N-1}$ that solves (9.1) is\n",
        "\n",
        "$$\n",
        "u_{N-1}^\\star = \\frac{2\\pi-x_{N-1}}{2},\n",
        "$$\n",
        "\n",
        "and therefore\n",
        "\n",
        "$$\n",
        "J_{N-1}(x_{N-1}) = 4 \\sin\\left(\\tfrac{2\\pi-x_{N-1}}{4}\\right).\n",
        "$$\n",
        "\n",
        "(Hint: the point at which a concave function’s gradient vanishes must be the unique maximizer of that function.)\n",
        "\n",
        "**(TODO) ANSWER:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "463c9458",
      "metadata": {
        "id": "463c9458"
      },
      "source": [
        "\n",
        "\n",
        "**d**. Now use induction to show that the $k$-th step dynamic programming\n",
        "\n",
        "$$\n",
        "J_k(x_k)\n",
        "= \\max_{0 < u_k < 2\\pi — x_k}\n",
        "\\left\\{ 2 \\sin\\left(\\tfrac{u_k}{2}\\right) + J_{k+1}(x_k + u_k) \\right\\}\n",
        "$$\n",
        "\n",
        "admits an optimal control\n",
        "\n",
        "$$\n",
        "u_k^\\star = \\frac{2\\pi-x_k}{N-k + 1},\n",
        "$$\n",
        "\n",
        "and optimal cost-to-go\n",
        "\n",
        "$$\n",
        "J_k(x_k) = 2 (N-k + 1) \\, \\sin\\!\\left( \\frac{2\\pi-x_k}{2 (N-k + 1)} \\right).\n",
        "$$\n",
        "\n",
        "**(TODO) ANSWER:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ad8ec40",
      "metadata": {
        "id": "0ad8ec40"
      },
      "source": [
        "\n",
        "\n",
        "**e**. Starting from $x_1 = 0$, what is the optimal sequence of controls?\n",
        "\n",
        "Hopefully now you see why my original claim is true!\n",
        "\n",
        "**(TODO) ANSWER:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33dfd1d1",
      "metadata": {
        "id": "33dfd1d1"
      },
      "source": [
        "### Bonus:\n",
        "We are not yet done for this exercise. Since you have probably already spent quite some time on this exercise, I will leave the rest of the exercise a bonus. In case you found this simple geometric problem interesting, you should keep reading as we will use numerical techniques to prove the same claim.\n",
        "\n",
        "In Fig. 2, by denoting\n",
        "\n",
        "$$\n",
        "u_N = 2\\pi - x_N = 2\\pi - (u_1 + \\cdots + u_{N-1})\n",
        "$$\n",
        "\n",
        "as the angle between the line $O — x_N$ and the line $O — x_1$, it is not hard to observe that the perimeter of the $N$-polygon is\n",
        "\n",
        "$$\n",
        "\\sum_{k=1}^N 2 \\sin\\!\\left(\\tfrac{u_k}{2}\\right).\n",
        "$$\n",
        "\n",
        "Consequently, to maximize the perimeter, we can formulate the following optimization\n",
        "\n",
        "$$\n",
        "\\max_{u_1,\\ldots,u_N} \\;\\; \\sum_{k=1}^N 2 \\sin\\!\\left(\\tfrac{u_k}{2}\\right)\n",
        "$$\n",
        "\n",
        "subject to\n",
        "\n",
        "$$\n",
        "u_k > 0, \\; k = 1, \\ldots, N, \\\\\n",
        "u_1 + \\cdots + u_N = 2\\pi\n",
        "\\tag{9.2}\n",
        "$$\n",
        "\n",
        "where $u_k$ can be seen as the angle spanned by the line $x_k — x_{k+1}$ with respect to the center $O$ so that they are positive and sum up to $2\\pi$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b21f4bb8",
      "metadata": {
        "id": "b21f4bb8"
      },
      "source": [
        "\n",
        "\n",
        "**f**. Show that the optimization (9.2) is convex.\n",
        "(Hint: first show the feasible set is convex, and then show the objective function is concave over the feasible set.)\n",
        "\n",
        "**(TODO) ANSWER:**\n",
        "\n",
        "Now that we have shown (9.2) is a convex optimization problem, we know that pretty much any numerical algorithm will guarantee convergence to the globally optimal solution.\n",
        "\n",
        "There are many numerical algorithms that can compute optimal solutions of an optimization problem (Nocedal and Wright 1999). Python provides a nice interface, `scipy.optimize`, to many such algorithms, and let us use `scipy.optimize` to solve (9.2) so we can numerically prove our claim."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5d86a56",
      "metadata": {
        "id": "c5d86a56"
      },
      "source": [
        "\n",
        "**g**. We have provided most of the code necessary for solving (9.2) below. Please fill in the definition of the function `perimeter(u)`, and then run the code. Show your results for $N = 3, 10, 100$. Do the solutions obtained from Python verify our claim?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9ce6c03",
      "metadata": {
        "id": "c9ce6c03"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------- Parameters --------\n",
        "N = 10  # Number of points\n",
        "\n",
        "# -------- Objective: polygon perimeter on the unit circle (edge length = 2*sin(u/2)) --------\n",
        "def perimeter(u):\n",
        "    ##############################\n",
        "    # TODO BLOCK\n",
        "    ##############################\n",
        "\n",
        "def neg_perimeter(u):\n",
        "# SciPy minimizes; negate to perform maximization\n",
        "    return -perimeter(u)\n",
        "\n",
        "# -------- Constraints & initialization --------\n",
        "# Linear equality: sum(a) = 2π\n",
        "eq_cons = {'type': 'eq', 'fun': lambda u: np.sum(u) - 2.0 * np.pi}\n",
        "\n",
        "# Bounds: u_i ∈ [0, 2π] (upper bound helps numerics)\n",
        "bounds = [(0.0, 2.0 * np.pi)] * N\n",
        "\n",
        "# Initial guess: positive random vector normalized to 2π\n",
        "rng = np.random.default_rng(0)\n",
        "u0 = rng.random(N)\n",
        "u0 = u0 / u0.sum() * 2.0 * np.pi\n",
        "\n",
        "# -------- Solve (SLSQP) --------\n",
        "res = minimize(\n",
        "    neg_perimeter, u0,\n",
        "    method='SLSQP',\n",
        "    bounds=bounds,\n",
        "    constraints=[eq_cons],\n",
        "    options={'maxiter': 2000, 'ftol': 1e-12, 'disp': True}\n",
        ")\n",
        "\n",
        "uopt = res.x\n",
        "print(\"Success:\", res.success, \"| message:\", res.message)\n",
        "print(\"Perimeter =\", perimeter(uopt))\n",
        "\n",
        "# -------- Recover vertex angles x by cumulative sum (x[0]=0; others accumulate preceding gaps) --------\n",
        "x = np.zeros(N)\n",
        "x[1:] = np.cumsum(uopt[:-1])\n",
        "\n",
        "# -------- Plot --------\n",
        "fig, ax = plt.subplots()\n",
        "# Draw unit circle\n",
        "circle = plt.Circle((0, 0), 1.0, fill=False)\n",
        "ax.add_patch(circle)\n",
        "\n",
        "# Scatter vertices\n",
        "ax.scatter(np.cos(x), np.sin(x), s=40, label=\"points\")\n",
        "\n",
        "ax.set_aspect('equal', adjustable='box')\n",
        "ax.set_xlim(-1.1, 1.1)\n",
        "ax.set_ylim(-1.1, 1.1)\n",
        "ax.set_xlabel(\"x\")\n",
        "ax.set_ylabel(\"y\")\n",
        "ax.set_title(f\"N={N}, perimeter={perimeter(uopt):.6f}\")\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a181bad5",
      "metadata": {
        "id": "a181bad5"
      },
      "source": [
        "## 2. Convergence proof of Value iteration\n",
        "\n",
        "Let the Bellman optimality operator be\n",
        "$$\n",
        "(T^\\star V)(s)=\\max_a\\Big[\\,R(s,a)+\\gamma\\sum_{s'}P(s'|s,a)V(s')\\,\\Big],\n",
        "\\qquad \\gamma\\in[0,1).\n",
        "$$\n",
        "Let $V^\\star$ denote the optimal value function, i.e., $V^\\star=T^\\star V^\\star$.\n",
        "Value iteration is $V_{k+1}=T^\\star V_k$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "553b1668",
      "metadata": {
        "id": "553b1668"
      },
      "source": [
        "### 2.1 Contraction\n",
        "\n",
        "We first prove the operator is a $\\gamma$-**contraction**, i.e.\n",
        "$$\n",
        "||V_{k+1}-V^\\star|| \\leq \\gamma ||V_k-V^\\star||\n",
        "$$\n",
        "\n",
        "**(TODO) Answer:**\n",
        "\n",
        "We learned in class that we do value iteration s.t. we can\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90b835d5",
      "metadata": {
        "id": "90b835d5"
      },
      "source": [
        "### 2.2 linear convergence\n",
        "Next we prove the convergence is actually **linear**, i.e.\n",
        "$$\n",
        "\\|V_k-V^\\star\\|_\\infty \\leq \\gamma^k \\|V_0-V^\\star\\|_\\infty\n",
        "$$\n",
        "\n",
        "**(TODO) Answer:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bd797a5",
      "metadata": {
        "id": "3bd797a5"
      },
      "source": [
        "### 2.3 Practical stopping rule\n",
        "\n",
        "In practice we never know what is the true $V^\\star$. But what we can calculate is the difference between two iterations. Here we (1) prove an error bound of $\\|V-V^\\star\\|_\\infty$ by $\\|V_{k+1} - V_k\\|_\\infty$:\n",
        "\n",
        "$$\n",
        "\\|V_k-V^\\star\\|_\\infty \\leq \\frac{\\|V_{k+1} - V_k\\|_\\infty}{1-\\gamma}\n",
        "$$\n",
        "\n",
        "and (2) Compute the tolerance on the consecutive-iterate gap $\\|V_{k+1}-V_k\\|_\\infty$ needed to guarantee $\\|V - V^\\star\\|_\\infty \\le 10^{-6}$ when $\\gamma=0.99$.\n",
        "\n",
        "**(TODO) Answer:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a065e0a",
      "metadata": {
        "id": "3a065e0a"
      },
      "source": [
        "### 2.4 The bound of iterations\n",
        "\n",
        "Assume $\\|V_1 - V_0\\|_\\infty = 1$, $\\gamma = 0.99$. How much iterations do we need to have $\\|V_k - V^\\star\\|_\\infty \\leq 10^{-6}$?\n",
        "\n",
        "**(TODO) Answer:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd169530",
      "metadata": {
        "id": "dd169530"
      },
      "source": [
        "## 3. Cliffwalk\n",
        "\n",
        "Implement policy evaluation, policy improvement, value iteration, and policy iteration for the `CliffWalking` task. For clarity and reproducibility, We include a minimal reimplementation of the environment that mirrors Gymnasium’s dynamics and reward scheme.\n",
        "\n",
        "\n",
        "<figure style=\"text-align:center;\">\n",
        "  <img src=\"https://raw.githubusercontent.com/ComputationalRobotics/2025-ES-AM-158-PSET/main/PSET1/cliffwalk.png\" width=\"600\" alt=\"Inscribed polygon\">\n",
        "  <figcaption style=\"color:#6a737d; font-style:italic;\">\n",
        "    Figure 3. Illustration to cliffwalk problem.\n",
        "  </figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5782a72c",
      "metadata": {
        "id": "5782a72c"
      },
      "source": [
        "**CliffWalking (Gym-compatible) — Specification**\n",
        "\n",
        "- **Grid:** 4 rows × 12 columns (row-major indexing; `state_id = row * 12 + col`; index origin at top-left in comments).\n",
        "- **Start:** bottom-left cell `(row=3, col=0)`.\n",
        "- **Goal:** bottom-right cell `(row=3, col=11)`.\n",
        "- **Actions (4):** up (0), right (1), down (2), left (3).\n",
        "- **Rewards:** −1 per step; −100 on entering a cliff cell; 0 at the goal.\n",
        "- **Termination:** episode ends upon reaching the goal; this states are terminal/absorbing. If reaching cliff will go back to start.\n",
        "\n",
        "**Transition table**\n",
        "\n",
        "- `P[state][action] → list[(prob, next_state, reward, done)]`\n",
        "- Deterministic dynamics: each list contains a single tuple with `prob = 1.0` after handling boundaries, cliff, and goal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3422e418",
      "metadata": {
        "id": "3422e418",
        "outputId": "d1ab896b-1cf2-4273-a0ff-ca55690a382d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "State count=48, Action count=4\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "# Create Gym CliffWalking environment (v1).\n",
        "env_gym = gym.make(\"CliffWalking-v1\", render_mode=\"ansi\")\n",
        "nS, nA = env_gym.observation_space.n, env_gym.action_space.n\n",
        "# The CliffWalking grid is 4 × 12; actions are 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT.\n",
        "print(f\"State count={nS}, Action count={nA}\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Utility: pretty-print a value function as a 2D grid (nrow × ncol).\n",
        "# Values can be any (nS,) array-like; states are indexed row-major:\n",
        "# State_id = row * ncol + col\n",
        "# -------------------------------------------------------------------\n",
        "def print_values(values, nrow: int, ncol: int, title: str = \"State Values\"):\n",
        "    \"\"\"Print a value table in grid form.\"\"\"\n",
        "    values = np.asarray(values).reshape(nrow, ncol)\n",
        "    print(title)\n",
        "    for r in range(nrow):\n",
        "        print(\" \".join(f\"{values[r, c]:6.2f}\" for c in range(ncol)))\n",
        "    print()\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Utility: pretty-print a policy on the CliffWalking grid.\n",
        "#\n",
        "# Accepted pi formats for each state s:\n",
        "# - Int a               : deterministic action\n",
        "# - Length-4 vector     : Q-values or preferences; we render argmax (ties shown)\n",
        "# - Length-4 probabilities (stochastic policy): greedy action(s) by max prob\n",
        "#\n",
        "# Notes:\n",
        "# - Uses Gym's action order: 0=UP(↑), 1=RIGHT(→), 2=DOWN(↓), 3=LEFT(←)\n",
        "# - Terminal states in CliffWalking (bottom row except col=0) are marked:\n",
        "# S at (last_row, 0), C for cliff cells (last_row, 1..ncol-2), G at (last_row, ncol-1)\n",
        "# -------------------------------------------------------------------\n",
        "def print_policy(pi, nrow: int, ncol: int, title: str = \"Policy\"):\n",
        "    \"\"\"Print a deterministic/stochastic policy.\n",
        "    - If pi is a list of lists (length 4): treat as stochastic over [up, down, left, right].\n",
        "    - We render the greedy direction; if ties exist, we list all best arrows.\n",
        "    \"\"\"\n",
        "    arrow = {0:\"^\", 1:\">\", 2:\"v\", 3:\"<\"}  # Order aligned with env actions in this notebook\n",
        "    print(title)\n",
        "    for i in range(nrow):\n",
        "        row_syms = []\n",
        "        for j in range(ncol):\n",
        "            s = i*ncol + j\n",
        "            p = pi[s]\n",
        "            # Determine best action(s)\n",
        "            if isinstance(p, list) and len(p) == 4:\n",
        "                best = np.argwhere(np.array(p) == np.max(p)).flatten().tolist()\n",
        "            elif isinstance(p, int):\n",
        "                best = [p]\n",
        "            else:\n",
        "                # Fallback: greedy over provided vector/array\n",
        "                arr = np.array(p, dtype=float).ravel()\n",
        "                best = np.argwhere(arr == np.max(arr)).flatten().tolist()\n",
        "            # Special case: terminals on bottom row except j==0\n",
        "            if i == nrow-1 and j > 0:\n",
        "                row_syms.append(\"T\")\n",
        "            else:\n",
        "                row_syms.append(\"\".join(arrow[a] for a in best))\n",
        "        print(\" \".join(sym if sym else \".\" for sym in row_syms))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc0f7f98",
      "metadata": {
        "id": "cc0f7f98"
      },
      "source": [
        "### 3.1 Define Environment Model (no need to fill in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48012a41",
      "metadata": {
        "id": "48012a41"
      },
      "outputs": [],
      "source": [
        "class CliffWalkingEnv:\n",
        "    \"\"\"Cliff Walking environment (Gym-compatible dynamics)\n",
        "\n",
        "    State indexing\n",
        "    --------------\n",
        "    - Flattened row-major: state_id = row * ncol + col\n",
        "    - Rows: 0..nrow-1 (top → bottom), Cols: 0..ncol-1 (left → right)\n",
        "\n",
        "    Actions (match Gym/toy_text)\n",
        "    ----------------------------\n",
        "    - 0: UP (↑), 1: RIGHT (→), 2: DOWN (↓), 3: LEFT (←)\n",
        "\n",
        "    Grid (nrow=4, ncol=12)\n",
        "    ----------------------\n",
        "        [  0] [  1] [  2] [  3] [  4] [  5] [  6] [  7] [  8] [  9] [ 10] [ 11]\n",
        "        [ 12] [ 13] [ 14] [ 15] [ 16] [ 17] [ 18] [ 19] [ 20] [ 21] [ 22] [ 23]\n",
        "        [ 24] [ 25] [ 26] [ 27] [ 28] [ 29] [ 30] [ 31] [ 32] [ 33] [ 34] [ 35]\n",
        "        [36=S] [37=C] [38=C] [39=C] [40=C] [41=C] [42=C] [43=C] [44=C] [45=C] [46=C] [47=G]\n",
        "\n",
        "    Legend\n",
        "    ------\n",
        "    - S (start):  (row=3, col=0)   -> state 36\n",
        "    - C (cliff):  (row=3, col=1..10) -> states 37..46\n",
        "    - G (goal):   (row=3, col=11)  -> state 47\n",
        "\n",
        "    Termination & rewards\n",
        "    ---------------------\n",
        "    - Stepping into a cliff cell: reward = -100, done = False, go back to start\n",
        "    - Any other move:             reward = -1,   done = False\n",
        "    - Terminal states are absorbing: once in {goal}, any action keeps you there with reward 0.\n",
        "    \"\"\"\n",
        "\n",
        "    # Action constants for clarity\n",
        "    A_UP, A_RIGHT, A_DOWN, A_LEFT = 0, 1, 2, 3\n",
        "\n",
        "    def __init__(self, ncol: int = 12, nrow: int = 4):\n",
        "        self.ncol = int(ncol)\n",
        "        self.nrow = int(nrow)\n",
        "        self.nS = self.nrow * self.ncol\n",
        "        self.nA = 4\n",
        "        # Transition table: P[state][action] = [(prob, next_state, reward, done)]\n",
        "        self.P = self._create_P()\n",
        "\n",
        "    def _create_P(self):\n",
        "    # Allocate empty transition table\n",
        "        P = [[[] for _ in range(self.nA)] for _ in range(self.nS)]\n",
        "\n",
        "        # Movement deltas in (dx, dy), matching action order: 0↑, 1→, 2↓, 3←\n",
        "        # NOTE: x increases to the right (columns), y increases downward (rows).\n",
        "        deltas = {\n",
        "            self.A_UP:    ( 0, -1),\n",
        "            self.A_RIGHT: ( 1,  0),  # (1, 0) Written to hint order; same as (1, 0)\n",
        "            self.A_DOWN:  ( 0,  1),\n",
        "            self.A_LEFT:  (-1,  0),\n",
        "        }\n",
        "\n",
        "        start_s = (self.nrow - 1) * self.ncol + 0\n",
        "        goal_s  = (self.nrow - 1) * self.ncol + (self.ncol - 1)\n",
        "\n",
        "        for r in range(self.nrow):\n",
        "            for c in range(self.ncol):\n",
        "                s = r * self.ncol + c\n",
        "\n",
        "                if r == self.nrow - 1 and c > 0:\n",
        "                    for a in range(self.nA):\n",
        "                        P[s][a] = [(1.0, s, 0.0, True)]\n",
        "                    continue\n",
        "\n",
        "                for a in range(self.nA):\n",
        "                    dx, dy = deltas[a]\n",
        "\n",
        "                    nc = min(self.ncol - 1, max(0, c + dx))\n",
        "                    nr = min(self.nrow - 1, max(0, r + dy))\n",
        "\n",
        "                    ns = nr * self.ncol + nc\n",
        "                    reward = -1.0\n",
        "                    done = False\n",
        "\n",
        "                    if nr == self.nrow - 1 and 1 <= nc <= self.ncol - 2:\n",
        "                        ns = start_s\n",
        "                        reward = -100.0\n",
        "                        done = False\n",
        "\n",
        "                    elif nr == self.nrow - 1 and nc == self.ncol - 1:\n",
        "                        done = True\n",
        "\n",
        "                    P[s][a] = [(1.0, ns, reward, done)]\n",
        "\n",
        "        return P\n",
        "\n",
        "\n",
        "# Build environment\n",
        "env = CliffWalkingEnv(ncol=12, nrow=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "572fedfe",
      "metadata": {
        "id": "572fedfe"
      },
      "source": [
        "### 3.2 Policy Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72beca54",
      "metadata": {
        "id": "72beca54"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(env, pi, gamma=0.95, theta=1e-10):\n",
        "    \"\"\"Iterative policy evaluation for a given stochastic policy π(a|s).\n",
        "\n",
        "    Args:\n",
        "        env: environment with a tabular transition model env.P where\n",
        "             P[s][a] = [(prob, next_state, reward, done)].\n",
        "        pi:  policy probabilities shaped [nS][4]; each pi[s] is a length-4 list\n",
        "             over actions [UP, RIGHT, DOWN, LEFT].\n",
        "        gamma: discount factor ∈ [0, 1).\n",
        "        theta: convergence threshold on the ∞-norm of value updates.\n",
        "\n",
        "    Returns:\n",
        "        v: list of state values of length nS.\n",
        "    \"\"\"\n",
        "    nS = env.nrow * env.ncol\n",
        "    v = [0.0] * nS  # Initialize V(s)=0\n",
        "    it = 1  # Iteration counter (logging only)\n",
        "\n",
        "    while True:\n",
        "        max_diff = 0.0\n",
        "        new_v = [0.0] * nS\n",
        "\n",
        "        for s in range(nS):\n",
        "            v_sum = 0.0  # Σ_a π(a|s) * Q(s,a)\n",
        "            for a in range(4):\n",
        "                #################################\n",
        "                # TODO: implement policy evaluation here\n",
        "                #################################\n",
        "\n",
        "            new_v[s] = v_sum\n",
        "            max_diff = max(max_diff, abs(new_v[s] - v[s]))\n",
        "\n",
        "        v = new_v\n",
        "        if max_diff < theta:\n",
        "            break\n",
        "        it += 1\n",
        "\n",
        "    print(f\"Policy evaluation converged in {it} iteration(s).\")\n",
        "    return v\n",
        "\n",
        "\n",
        "# --- Example: evaluate a uniform random policy ---\n",
        "pi = [[0.25, 0.25, 0.25, 0.25] for _ in range(env.nrow * env.ncol)]\n",
        "gamma = 0.95\n",
        "\n",
        "v = policy_evaluation(env, pi, gamma)\n",
        "\n",
        "# Pretty-print the value function as a 4×12 grid\n",
        "print_values(v, env.nrow, env.ncol, title=\"Value Function under Random Policy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da66ff59",
      "metadata": {
        "id": "da66ff59"
      },
      "source": [
        "### 3.3 Policy Iteration\n",
        "\n",
        "Policy Iteration alternates between:\n",
        "1) **Policy Evaluation**: compute the state-value function $V^{\\pi}$ of the current policy $\\pi$\n",
        "2) **Policy Improvement**: update $\\pi$ to be greedy w.r.t. $V^{\\pi}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37e91266",
      "metadata": {
        "id": "37e91266"
      },
      "outputs": [],
      "source": [
        "def policy_improvement(env, pi, v, gamma=0.95):\n",
        "    \"\"\"Greedy policy improvement w.r.t. the current state-value function V.\n",
        "\n",
        "    For each state s:\n",
        "      1) Compute Q(s,a) = Σ_{s'} P(s'|s,a)[ r(s,a,s') + γ V(s') ] for all a.\n",
        "      2) Find the action(s) with maximal Q(s,a).\n",
        "      3) Update π(·|s) to split probability uniformly among all maximizers (tie-aware).\n",
        "\n",
        "    Args:\n",
        "        env: Tabular environment with transitions env.P where\n",
        "             P[s][a] = [(prob, next_state, reward, done)].\n",
        "        pi:  Current (possibly stochastic) policy, shape [nS][4]; updated in-place.\n",
        "        v:   Current state-value function V(s), length nS.\n",
        "        gamma: Discount factor.\n",
        "\n",
        "    Returns:\n",
        "        pi: The improved policy (same object, updated in-place).\n",
        "    \"\"\"\n",
        "    nS = env.nrow * env.ncol\n",
        "    nA = 4\n",
        "    eps = 1e-8  # Numerical tolerance for tie-breaking\n",
        "\n",
        "    for s in range(nS):\n",
        "        q_list = []\n",
        "        for a in range(nA):\n",
        "            #################################\n",
        "            # TODO: implement policy iteration here\n",
        "            #################################\n",
        "\n",
        "        pi[s] =\n",
        "\n",
        "    print(\"Policy improvement completed.\")\n",
        "    return pi\n",
        "\n",
        "\n",
        "# --- Policy Iteration loop ---\n",
        "pi = [[0.25, 0.25, 0.25, 0.25] for _ in range(env.nrow * env.ncol)]\n",
        "iters = 0\n",
        "while True:\n",
        "    v = # TODO: add policy evaluation\n",
        "    old_pi = copy.deepcopy(pi)\n",
        "    new_pi = # TODO: add policy improvement\n",
        "    iters += 1\n",
        "    if old_pi == new_pi:  # Policy is stable\n",
        "        print(f\"Policy iteration converged in {iters} improvement step(s).\")\n",
        "        break\n",
        "\n",
        "# Report results\n",
        "print_values(v, env.nrow, env.ncol, title=\"Optimal Value Function\")\n",
        "print_policy(pi, env.nrow, env.ncol, title=\"Optimal Policy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "569b7f8d",
      "metadata": {
        "id": "569b7f8d"
      },
      "source": [
        "### 3.4 Value Iteration\n",
        "\n",
        "Value Iteration applies **Bellman optimality** updates directly to $V$. Or one can treat value iteration as one step policy evaluation plus one step policy improvement.\n",
        "\n",
        "After convergence, extract the greedy policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4065588e",
      "metadata": {
        "id": "4065588e"
      },
      "outputs": [],
      "source": [
        "def iterate(env, gamma=0.95, theta=1e-10):\n",
        "    \"\"\"Value Iteration.\n",
        "\n",
        "    Updates V(s) <- max_a Σ_{s'} P(s'|s,a) [ r(s,a,s') + γ V(s') ]\n",
        "    until the maximum state-wise change is below `theta`.\n",
        "\n",
        "    Args:\n",
        "        env: Tabular environment exposing env.P with\n",
        "             P[s][a] = [(prob, next_state, reward, done)] and grid sizes nrow, ncol.\n",
        "        gamma (float): Discount factor in [0, 1).\n",
        "        theta (float): Convergence threshold on the infinity-norm of value updates.\n",
        "\n",
        "    Returns:\n",
        "        list[float]: The converged state-value function V of length nS (= nrow * ncol).\n",
        "\n",
        "    Notes:\n",
        "        - Terminal states are modeled as absorbing with reward 0 in `env.P`.\n",
        "          The Bellman backup naturally yields V(terminal) = 0.\n",
        "        - `deltas` (max per-iteration change) is tracked for debugging but not returned.\n",
        "    \"\"\"\n",
        "    nS, nA = env.nrow * env.ncol, 4\n",
        "    deltas = []\n",
        "    iters = 0\n",
        "    v = [0.0] * nS\n",
        "\n",
        "    while True:\n",
        "        iters += 1\n",
        "        max_diff = 0.0\n",
        "        new_v = [0.0] * nS\n",
        "\n",
        "        for s in range(nS):\n",
        "            # Bellman optimality backup: V(s) = max_a Q(s,a)\n",
        "            q_list = []\n",
        "            for a in range(nA):\n",
        "                #################################\n",
        "                # TODO: implement policy evaluation here\n",
        "                #################################\n",
        "            max_diff = max(max_diff, abs(new_v[s] - v[s]))\n",
        "\n",
        "        v = new_v\n",
        "        deltas.append(max_diff)\n",
        "        if max_diff < theta:\n",
        "            break\n",
        "    print(iters)\n",
        "    return v\n",
        "\n",
        "\n",
        "def greedy_policy(env, v, gamma=0.95):\n",
        "    \"\"\"Extract a greedy (tie-aware) policy from a value function.\n",
        "\n",
        "    For each state s, compute Q(s,a) and set π(a|s)=1/k for all actions a that\n",
        "    achieve the maximal Q-value (ties split uniformly); 0 otherwise.\n",
        "\n",
        "    Args:\n",
        "        env: Tabular environment with env.P.\n",
        "        v (list[float]): State-value function V(s).\n",
        "        gamma (float): Discount factor.\n",
        "\n",
        "    Returns:\n",
        "        list[list[float]]: Policy π of shape [nS][4], each row summing to 1.\n",
        "    \"\"\"\n",
        "    nS, nA = env.nrow * env.ncol, 4\n",
        "    pi = [[0.0] * nA for _ in range(nS)]\n",
        "    eps = 1e-8  # Numerical tolerance for tie detection\n",
        "\n",
        "    for s in range(nS):\n",
        "        q_list = []\n",
        "        for a in range(nA):\n",
        "            q = 0.0\n",
        "            for (p, next_state, r, done) in env.P[s][a]:\n",
        "                q += p * (r if done else r + gamma * v[next_state])\n",
        "            q_list.append(q)\n",
        "\n",
        "        q_list = np.array(q_list, dtype=float)\n",
        "        max_q = q_list.max()\n",
        "        # Tie-aware argmax\n",
        "        opt_u = np.isclose(q_list, max_q, rtol=0.0, atol=eps)\n",
        "        k = int(opt_u.sum())\n",
        "        pi[s] = (opt_u / k).astype(float).tolist()\n",
        "\n",
        "    return pi\n",
        "\n",
        "\n",
        "# ----- Run Value Iteration and extract greedy policy -----\n",
        "gamma = 0.95  # Discount factor\n",
        "v = iterate(env, gamma=gamma)  # Assumes `env` is already constructed\n",
        "pi = greedy_policy(env, v, gamma=gamma)\n",
        "\n",
        "# Pretty-print results (assumes `print_values` and `print_policy` are defined)\n",
        "print_values(v, env.nrow, env.ncol, title=\"Optimal Value Function (Value Iteration)\")\n",
        "print_policy(pi, env.nrow, env.ncol, title=\"Optimal Policy (Value Iteration)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e865218",
      "metadata": {
        "id": "4e865218"
      },
      "source": [
        "## 4. Matrix–vector Representation of DP\n",
        "\n",
        "We’ll use a small finite MDP ($|X|=16$, $|A|=4$) and **matrix** forms to compute the optimal $Q$-function:\n",
        "\n",
        "4.1. Build the **transition matrix**\n",
        "   $\n",
        "   P \\in \\mathbb{R}^{|X||A|\\times |X|}\n",
        "   $\n",
        "   and the **immediate reward** vector\n",
        "   $\n",
        "   r \\in \\mathbb{R}^{|X||A|}.\n",
        "   $\n",
        "\n",
        "4.2. Using the matrix form of the $Q$-value function $Q_\\pi$ and the value function $V_\\pi$ to write down the bellman equation.\n",
        "\n",
        "4.3. Define the **Bellman optimality operator**:\n",
        "   $$\n",
        "   T^\\star(Q) = r + \\gamma\\, P\\, J_Q,\n",
        "   $$\n",
        "   where\n",
        "   $$\n",
        "   (J_Q)(x) = \\max_{a} Q(x,a).\n",
        "   $$\n",
        "   Iterating $Q_{k+1} = T^\\star(Q_k)$ converges to the optimal $Q^\\star$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35440cec",
      "metadata": {
        "id": "35440cec"
      },
      "source": [
        "### 4×4 Gridworld — From Bottom‑Left (Start) to Top‑Right (Goal)\n",
        "\n",
        "**States:** 16 cells in a 4×4 grid, row-major indexing with top-left as (row=0, col=0).\n",
        "State id: `s = row * 4 + col`, rows increase downward.\n",
        "\n",
        "**Start:** bottom-left `(row=3, col=0)` → `s_start = 12`\n",
        "**Goal:** top-right `(row=0, col=3)` → `s_goal = 3`\n",
        "\n",
        "**Actions (4):**\n",
        "- `a=0` → UP (↑)\n",
        "- `a=1` → RIGHT (→)\n",
        "- `a=2` → DOWN (↓)\n",
        "- `a=3` → LEFT (←)\n",
        "\n",
        "**Dynamics:** Deterministic. If an action would leave the grid world, the agent stays in place.\n",
        "\n",
        "**Rewards (maximize):**\n",
        "- `-1` per step\n",
        "- `0` in the goal\n",
        "\n",
        "**Terminal:** The goal is absorbing (from goal, any action keeps you at goal with reward 0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76ef97dd",
      "metadata": {
        "id": "76ef97dd",
        "outputId": "2ade01cd-b241-4d98-ff30-0e49229d63b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grid 4×4. Start=S (row=3,col=0), Goal=G (row=0,col=3)\n",
            " 0  1  2  G \n",
            " 4  5  6  7\n",
            " 8  9 10 11\n",
            " S  13 14 15\n"
          ]
        }
      ],
      "source": [
        "# Grid size\n",
        "nrow, ncol = 4, 4\n",
        "nS = nrow * ncol  # |X| = 16\n",
        "nA = 4  # |A| = 4 (UP, RIGHT, DOWN, LEFT)\n",
        "\n",
        "# Start (bottom-left) and Goal (top-right)\n",
        "s_start = (nrow - 1) * ncol + 0  # 12\n",
        "s_goal  = 0 * ncol + (ncol - 1)  # 3\n",
        "\n",
        "# Row-major state id\n",
        "def s_id(r, c):\n",
        "    return r * ncol + c\n",
        "\n",
        "# For state-action row index in matrices of shape (nS*nA, ...)\n",
        "def sa_id(s, a):\n",
        "    return s * nA + a\n",
        "\n",
        "# Action deltas: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
        "DELTAS = {\n",
        "    0: (-1,  0),  # UP:    row-1\n",
        "    1: ( 0,  1),  # RIGHT: col+1\n",
        "    2: ( 1,  0),  # DOWN:  row+1\n",
        "    3: ( 0, -1),  # LEFT:  col-1\n",
        "}\n",
        "\n",
        "# Quick sanity checks and a tiny ASCII map\n",
        "print(\"Grid 4×4. Start=S (row=3,col=0), Goal=G (row=0,col=3)\")\n",
        "for rrow in range(nrow):\n",
        "    line = []\n",
        "    for ccol in range(ncol):\n",
        "        s = s_id(rrow, ccol)\n",
        "        if s == s_start:\n",
        "            line.append(\" S \")\n",
        "        elif s == s_goal:\n",
        "            line.append(\" G \")\n",
        "        else:\n",
        "            line.append(f\"{s:2d}\")\n",
        "    print(\" \".join(line))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94e2d4d6",
      "metadata": {
        "id": "94e2d4d6"
      },
      "source": [
        "### 4.1 Build Transition Matrix and Reward Vector\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "- **Transition matrix** $P \\in \\mathbb{R}^{|X||A|\\times |X|}$\n",
        "  Rows index state–action pairs $(x,a)$, columns index next states $x'$.\n",
        "  Entry:\n",
        "  $$\n",
        "  P[(x,a),\\,x'] \\;\\equiv\\; \\Pr\\{X_{t+1}=x' \\mid X_t=x,\\; A_t=a\\}.\n",
        "  $$\n",
        "  Row-wise normalization holds: $\\sum_{x'} P[(x,a),x'] = 1$ for every $(x,a)$.\n",
        "\n",
        "- **Reward vector** $r \\in \\mathbb{R}^{|X||A|}$ (reward maximization form)\n",
        "  Each entry is the one-step expected reward under $(x,a)$:\n",
        "  $$\n",
        "  r[(x,a)] \\;\\equiv\\; \\mathbb{E}\\big[\\,R_{t+1}\\mid X_t=x,\\; A_t=a\\,\\big].\n",
        "  $$\n",
        "\n",
        "**Indexing note.**\n",
        "A convenient index for $(x,a)$ is\n",
        "$$\n",
        "i = x\\,|A| + a\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86995a5d",
      "metadata": {
        "id": "86995a5d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Build P (|X||A| × |X|) and r (|X||A|)\n",
        "P = np.zeros((nS * nA, nS), dtype=float)\n",
        "r = np.zeros(nS * nA, dtype=float)\n",
        "\n",
        "for s in range(nS):\n",
        "# This will give // and %\n",
        "    r0, c0 = divmod(s, ncol)\n",
        "\n",
        "    for a in range(nA):\n",
        "        _s_id = sa_id(s, a)\n",
        "\n",
        "        # Goal is absorbing with reward 0\n",
        "        if s == s_goal:\n",
        "            P[_s_id, s_goal] = # TODO ...\n",
        "            r[_s_id] = # TODO ...\n",
        "            continue\n",
        "\n",
        "        dr, dc = DELTAS[a]\n",
        "        rr = min(nrow - 1, max(0, r0 + dr))\n",
        "        cc = min(ncol - 1, max(0, c0 + dc))\n",
        "        s_next = s_id(rr, cc)\n",
        "\n",
        "        # Deterministic transition\n",
        "        P[_s_id, s_next] = # TODO ...\n",
        "\n",
        "        # Reward: -1 per step, 0 in goal (already handled above)\n",
        "        r[_s_id] = # TODO ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d5a23e0",
      "metadata": {
        "id": "8d5a23e0"
      },
      "source": [
        "### 4.2 Matrix Form of Bellman Consistency and Bellman equation\n",
        "\n",
        "Q-evaluation when a fixed policy $\\pi$ is given:\n",
        "\n",
        "$$\n",
        "Q_\\pi(x,a) = r(x,a) + \\gamma \\,\\mathbb{E}_{x'\\sim P(\\cdot \\mid x,a)} \\, V_\\pi(x') \\tag{4.3(1)}\n",
        "$$\n",
        "\n",
        "The bellman equation:\n",
        "$$\n",
        "Q^\\star(x,a) \\;=\\; r(x,a) \\;+\\; \\gamma \\, \\mathbb{E}_{x' \\sim P(\\cdot \\mid x,a)}\n",
        "\\left\\{ \\max_{a' \\in A} Q^\\star(x',a') \\right\\},\n",
        "\\qquad \\forall (x,a) \\in X \\times A. \\tag{4.3(2)}\n",
        "$$\n",
        "\n",
        "where $Q^\\star$ is the optimal $Q$-value function. Similarly, let us define\n",
        "\n",
        "$$\n",
        "J_Q(x) = \\max_{a \\in A} Q(x,a).\n",
        "$$\n",
        "\n",
        "Question: How to write these equations (4.3(1))&(2) in matrix and operator form?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "570aac90",
      "metadata": {
        "id": "570aac90"
      },
      "source": [
        "**(TODO) Answer:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "054a258d",
      "metadata": {
        "id": "054a258d"
      },
      "source": [
        "### 4.3 Solve bellman equation.\n",
        "\n",
        "Note that $J_Q$ has dimension $|X|$. With these notations, the *Bellman optimality operator* is defined as\n",
        "\n",
        "$$\n",
        "T^\\star Q \\;=\\; g + \\gamma P J_Q,\n",
        "\\tag{2.27}\n",
        "$$\n",
        "\n",
        "which is nothing but a matrix representation of the right-hand side of Bellman equation.\n",
        "This allows us to concisely write the Bellman equation as\n",
        "\n",
        "$$\n",
        "Q = T^\\star Q.\n",
        "\\tag{2.28}\n",
        "$$\n",
        "\n",
        "One can do to solve this equation is through *fix-point iteration*:\n",
        "$$\n",
        "Q_{n+1} = T^\\star Q_n.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b44ffe2",
      "metadata": {
        "id": "1b44ffe2"
      },
      "outputs": [],
      "source": [
        "Q = np.zeros((nS * nA), dtype=float)\n",
        "for i in range(1000):\n",
        "    old_Q = Q.copy()\n",
        "    J_Q =  # TODO: V(s) = max_a Q(s,a)\n",
        "    Q = # TODO ...\n",
        "    if np.max(np.abs(Q - old_Q)) < 1e-10:\n",
        "        print(f\"Converged in {i+1} iterations.\")\n",
        "        break\n",
        "J_Q = Q.reshape(nS, nA).max(axis=1)\n",
        "print(\"\\nOptimal state values J_Q (V*) on the grid:\")\n",
        "for r0 in range(nrow):\n",
        "    row_vals = []\n",
        "    for c0 in range(ncol):\n",
        "        s = r0 * ncol + c0\n",
        "        if s == s_start:\n",
        "            row_vals.append(\" S \")\n",
        "        elif s == s_goal:\n",
        "            row_vals.append(\" G \")\n",
        "        else:\n",
        "            row_vals.append(f\"{J_Q[s]:6.2f}\")\n",
        "    print(\" \".join(row_vals))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "2025ocrl-pset",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}